{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) & [requests documentation](http://docs.python-requests.org/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put the HTML structure into the variable `r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"http://www.eib.org/infocentre/register/all/index.htm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a `soup`- prepares the HTML to be parsed by BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = bs(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BS basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are some basic operations you will use when scraping with BeautifuSoup.\n",
    "`.title` will give you the title of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title lang=\"en\">Basic search</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are often not really interested in the tags (`<title>`), but in the text itself. Easy enough! Let's just put `.text` behind the expression. You can use `.text` to leave out the tags of every expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Basic search'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the text of the first paragraph on the page? For more examples [see the documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Skip to navigation'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our EIB example, we want to extract links from a table. We need to specify which table in order to get the right links (and not just any links on the page). Right click + `inspect element` on a page shows us the HTML structure. We see that the table we are interested in has `class = datatable`. This is how we tell BeautifulSoup to find the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", {\"class\" :\"datatable\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before extracting the links, let's create a variable that will \"hold\" the links first. Brackets `[]` indicate a list. [Click here for more information on what is a list](http://www.tutorialspoint.com/python/python_lists.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set! Let's find all the links in the table with `table.find_all('a')`. Do you remember how the HTML structure of a link looks like?\n",
    "\n",
    "`<a href=\"http://herebethelink.com/this-you-dont-see\">This is what you see</a>`\n",
    "\n",
    "We extract the `href` attribute from `a` by `link.get('href')`. Here `link` is each `a` BeautifulSoup encounters in `table.find_all('a')`. We then replace the `htm` with `pdf`, as we know that that is the link to the documents we are after. We then put all the links into the list we created by `.append`. Please see more about [list methods](###) in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for link in table.find_all('a'):\n",
    "    pdfs.append(link.get('href').replace(\"htm\", \"pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a peak at the first 10 items of the list we have created. Read more about [slicing lists](http://www.tutorialspoint.com/python/python_lists.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/infocentre/register/help/result-page/index',\n",
       " '/infocentre/register/all/66903378.pdf',\n",
       " '/infocentre/register/all/66903378.pdf',\n",
       " '/infocentre/register/all/66883999.pdf',\n",
       " '/infocentre/register/all/66883999.pdf',\n",
       " '/infocentre/register/all/66889675.pdf',\n",
       " '/infocentre/register/all/66889675.pdf',\n",
       " '/infocentre/register/all/66882234.pdf',\n",
       " '/infocentre/register/all/66882234.pdf',\n",
       " '/infocentre/register/all/66900854.pdf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, some of the links are dubble in there. They were also double in the table. That is fine. We just need to use the method `set()`. That removes all the duplicates from the list. Try to do **that** in excel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfs_clean = set(pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first check the first 20 links we have cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdfs_clean[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks good! Lets scrape now! There is a lot happening in this bit, so let's have a look at it bit by bit.\n",
    "\n",
    "`with open(*something*) as *someting* ` is a special expression we use when opening files for writing. Everything that comes underneath this expression happens while the file is still open. As soon as we go out of the loop, the file closes. The `as` part is basically the same as above when we renamed BeautifulSoup into `bs`.\n",
    "\n",
    "`open()` opens up a file in Python. `\"w\"` indicates the file has to be opened for writing. See more about [opening, reading and writing files](###).\n",
    "\n",
    "`.split()` splits a string (text) to a list according to another piece of text. In our case, we split the url we have on `/` . This gives us a list, e.g. ### . \n",
    "\n",
    "`[-1]` is again slicing (see above), but reversed. Instead of giving us the first element from the front, we grab the first one from the back, which we make our filename.\n",
    "\n",
    "`write()` writes to a file\n",
    "`requests.get(*something*).content` gets content of a non-text response (remember that webpage = HTML is a text response! A PDF is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pdf in pdfs_clean:\n",
    "    with open(pdf.split(\"/\")[-1], \"w\") as output:\n",
    "        output.write(requests.get(\"http://www.eib.org\"+pdf).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set! Keep it up, scrape, read about how to do it, use [StackOverflow](http://stackoverflow.com/), try it out and make many mistakes in order to learn more. Happy scraping!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
